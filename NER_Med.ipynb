{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c8f320-9509-4c0c-b2d0-1afc81363796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import torch\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332227cd-ef97-4ce9-b33d-4c1abf81223c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_medmentions():\n",
    "    url = \"https://github.com/chanzuckerberg/MedMentions/blob/master/full/data/corpus_pubtator.txt.gz?raw=true\"\n",
    "    filename = \"./corpus/corpus_pubtator.txt.gz\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    \n",
    "    if not os.path.exists(\"./corpus/corpus_pubtator.txt\"):\n",
    "        print(\"Extracting dataset...\")\n",
    "        with gzip.open(filename, 'rb') as f_in:\n",
    "            with open('corpus_pubtator.txt', 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541a3940-c08d-495f-8102-79ad3beb3ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_medmentions(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read().strip().split('\\n\\n')\n",
    "    \n",
    "    data = []\n",
    "    for entry in tqdm(content, desc=\"Reading entries\"):\n",
    "        lines = entry.split('\\n')\n",
    "        title = lines[0].split('|t|')[1]\n",
    "        abstract = lines[1].split('|a|')[1]\n",
    "        text = title + ' ' + abstract\n",
    "        entities = [line.split('\\t') for line in lines[2:] if len(line.split('\\t')) > 1]\n",
    "        data.append((text, entities))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d308702e-a5d6-4c6b-a720-04167408c41c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_bio_tags(text, entities):\n",
    "    words = text.split()\n",
    "    tags = ['O'] * len(words)\n",
    "    \n",
    "    for entity in entities:\n",
    "        start, end, entity_type, umls_id = int(entity[0]), int(entity[1]), entity[2], entity[3]\n",
    "        start_word = len(text[:start].split())\n",
    "        end_word = len(text[:end].split())\n",
    "        \n",
    "        # Filter for symptom-related semantic types\n",
    "        symptom_types = ['sosy', 'patf', 'dsyn', 'fndg']\n",
    "        if any(st in entity_type.lower() for st in symptom_types):\n",
    "            tags[start_word] = 'B-SYMPTOM'\n",
    "            for i in range(start_word + 1, end_word):\n",
    "                tags[i] = 'I-SYMPTOM'\n",
    "    \n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbda47ad-91eb-4bb6-a47e-a4c5172a8c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_medmentions_data():\n",
    "    download_medmentions()\n",
    "    \n",
    "    all_data = read_medmentions('corpus_pubtator.txt')\n",
    "    \n",
    "    processed_data = []\n",
    "    for text, entities in tqdm(all_data, desc=\"Processing entries\"):\n",
    "        words, tags = create_bio_tags(text, entities)\n",
    "        processed_data.append((words, tags))\n",
    "    \n",
    "    df = pd.DataFrame(processed_data, columns=['text', 'labels'])\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join(x))\n",
    "    df['labels'] = df['labels'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_df.to_csv('./data/medmentions_train.csv', index=False)\n",
    "    val_df.to_csv('./data/medmentions_val.csv', index=False)\n",
    "    test_df.to_csv('./data/medmentions_test.csv', index=False)\n",
    "    \n",
    "    print(f\"Saved {len(train_df)} training samples, {len(val_df)} validation samples, and {len(test_df)} test samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fdb071a-911a-4509-aa8e-7f632420d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_medical_dataset(file_path, max_seq_length=128):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Assuming your CSV has columns: 'text' and 'labels'\n",
    "    texts = df['text'].tolist()\n",
    "    \n",
    "    # Convert string labels to list of integers\n",
    "    labels = df['labels'].apply(lambda x: [int(label) for label in x.split()])\n",
    "    \n",
    "    # Pad or truncate labels to match max_seq_length\n",
    "    labels = [label + [0] * (max_seq_length - len(label)) if len(label) < max_seq_length \n",
    "              else label[:max_seq_length] for label in labels]\n",
    "    \n",
    "    return texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "623d54fe-c2a0-48ec-addb-2bcbae66c945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MedicalNERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1a34506-b06b-49b6-904a-633702d20dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a2a2a72-495a-4100-bff3-8f1d4ca2c1ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading entries: 100%|██████████| 4392/4392 [00:01<00:00, 2868.07it/s]\n",
      "Processing entries: 100%|██████████| 4392/4392 [00:07<00:00, 597.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3161 training samples, 352 validation samples, and 879 test samples.\n"
     ]
    }
   ],
   "source": [
    "prepare_medmentions_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4723b4d7-7fb2-4160-b8e9-7917250f7e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MedMentionsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_map = {'O': 0}  # Start with 'O' as 0\n",
    "        self.num_labels = 1  # Start with 1 for 'O'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = str(self.labels[item])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        # Convert string labels to integers\n",
    "        label_ids = self.convert_labels_to_ids(label.split())\n",
    "        \n",
    "        # Add -100 for special tokens\n",
    "        label_ids = [-100] + label_ids[:self.max_len-2] + [-100]\n",
    "        label_ids += [-100] * (self.max_len - len(label_ids))\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def convert_labels_to_ids(self, label_list):\n",
    "        ids = []\n",
    "        for label in label_list:\n",
    "            if label not in self.label_map:\n",
    "                self.label_map[label] = self.num_labels\n",
    "                self.num_labels += 1\n",
    "            ids.append(self.label_map[label])\n",
    "        return ids\n",
    "\n",
    "    def get_num_labels(self):\n",
    "        return self.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1ee3844-b3d1-413b-9f19-b327ad2fc43d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_medmentions_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['labels'].tolist()\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6695742-ff01-43e1-9b7c-9343ab4c7890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f0c3e64-5c21-4857-b12c-31dd9c58457d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=2)\n",
    "            \n",
    "            predictions.extend(preds[labels != -100].cpu().numpy())\n",
    "            true_labels.extend(labels[labels != -100].cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "362cc4ff-3ee7-4977-b6d6-11b791c993f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9744ed84-6e21-42be-9a9d-be7183d1c195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_texts, train_labels = load_medmentions_dataset('./data/medmentions_train.csv')\n",
    "val_texts, val_labels = load_medmentions_dataset('./data/medmentions_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5da7c3f3-71b4-405c-82e8-0ebeb4e7abc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hatim\\Code\\Sem 7\\Major-Project\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets and initialize tokenizers\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "train_dataset = MedMentionsDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_dataset = MedMentionsDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4959d345-619c-4e4b-93c7-da533fdf1f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "num_labels = train_dataset.get_num_labels()\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee2bee21-a954-4bc9-adfb-97502b88ddcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hatim\\Code\\Sem 7\\Major-Project\\venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f39b4-05ca-4a99-a60c-2f2acc6177e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device, scheduler)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "        \n",
    "    accuracy, precision, recall, f1 = evaluate(model, val_loader, device)\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Validation Precision: {precision:.4f}\")\n",
    "    print(f\"Validation Recall: {recall:.4f}\")\n",
    "    print(f\"Validation F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f36a59-78eb-41d3-8ee3-e046ea0d1421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss}\")\n",
    "    # add validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26ee31-bcfb-415c-8344-0f2a7693506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"./medical_ner_model\")\n",
    "tokenizer.save_pretrained(\"./medical_ner_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
